{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, validation_curve, cross_val_score\n",
    "from sklearn. linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from get_data import split_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Set random seed\n",
    "seed = 42"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import Cleaned Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = pd.read_csv('rhs_cleaned_dataset.csv')\n",
    "y = X.pop('max ultimate height')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=seed)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, random_state=seed)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "X.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   min ultimate height  min ultimate spread  max ultimate spread  \\\n",
       "0                  1.5                  1.5                  2.5   \n",
       "1                 12.0                  4.0                  8.0   \n",
       "2                 12.0                  4.0                  8.0   \n",
       "3                 12.0                  4.0                  8.0   \n",
       "4                  1.0                  1.0                  1.5   \n",
       "\n",
       "   min time to ultimate height  max time to ultimate height  \n",
       "0                         10.0                         20.0  \n",
       "1                         20.0                         50.0  \n",
       "2                         50.0                         50.0  \n",
       "3                         20.0                         50.0  \n",
       "4                          5.0                         10.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min ultimate height</th>\n",
       "      <th>min ultimate spread</th>\n",
       "      <th>max ultimate spread</th>\n",
       "      <th>min time to ultimate height</th>\n",
       "      <th>max time to ultimate height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Normalise data\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_scaler= scaler.fit(X_train)\n",
    "X_scaled = X_train_scaler.transform(X_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fit Linear Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Fit model\n",
    "linear_regression_model = LinearRegression(normalize=True)\n",
    "linear_regression_model.fit(X_train, y_train)\n",
    "\n",
    "print(f'Score on the training set is: {linear_regression_model.score(X_train, y_train)}')\n",
    "print(f'Score on the validation set is: {linear_regression_model.score(X_val, y_val)}')\n",
    "print(f'Linear regression coefficients are: {linear_regression_model.coef_}')\n",
    "\n",
    "\n",
    "# Check cross validation score on Validation set\n",
    "lin_reg_scores = cross_val_score(linear_regression_model, X_train, y_train, cv=10)\n",
    "print(f'cross validation scores: {lin_reg_scores}')\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (lin_reg_scores.mean(), lin_reg_scores.std()))\n",
    "\n",
    "y_pred = linear_regression_model.predict(X)\n",
    "print(f'predicted values: {y_pred}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score on the training set is: 0.9389720331472529\n",
      "Score on the validation set is: 0.9299984636179803\n",
      "Linear regression coefficients are: [ 0.84891818 -0.39056198  0.6814394  -0.00662977  0.00492183]\n",
      "cross validation scores: [0.93211118 0.95823971 0.91782921 0.93000995 0.94281591 0.94680839\n",
      " 0.9079411  0.94636185 0.91660568 0.9517921 ]\n",
      "0.94 accuracy with a standard deviation of 0.02\n",
      "predicted values: [ 2.74070312 14.50721297 14.30831997 ...  2.08831669  4.20514896\n",
      "  1.23939851]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Mean squared error score\n",
    "def mse_score(y_pred, y):\n",
    "    mse_score = round(mean_squared_error(y, y_pred, squared=True),2)\n",
    "    print(f'The mean squared error is {mse_score}')\n",
    "mse_score(y_pred, y)\n",
    "\n",
    "def calculate_loss(y_pred, y):\n",
    "    return np.mean((y_pred - y) ** 2)\n",
    "\n",
    "# Plot predictions with true labels\n",
    "def plot_predictions(y_pred, y):\n",
    "    samples = len(y_pred)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(np.arange(samples), y_pred, c='r', label='predictions')\n",
    "    plt.scatter(np.arange(samples), y, c='b', label='true labels', marker='x')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Sample numbers')\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()\n",
    "    \n",
    "calculate_loss(y_pred, y)\n",
    "plot_predictions(y_pred, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_linear_model(y_pred, y):\n",
    "    sample_y_pred = y_pred[:60]\n",
    "    sample_y = y[:60]\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(sample_y, label='True values')\n",
    "    plt.plot(sample_y_pred, label='Predicted values')\n",
    "    plt.xlabel('Number of samples')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend(prop=dict(size=10))\n",
    "    plt.title('Linear regression: comparison of predicted values with true values')\n",
    "plot_linear_model(y_pred, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "bayesian_model = BayesianRidge(compute_score=True)\n",
    "bayesian_model.fit(X_train, y_train)\n",
    "bayesian_y_pred = bayesian_model.predict(X)\n",
    "print(f'Predicted values: {bayesian_y_pred}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check cross validation score on TRAINING set\n",
    "bayesian_scores = cross_val_score(bayesian_model, X, y, cv=10)\n",
    "print('Bayesian regression cross validation scores:', bayesian_scores)\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (bayesian_scores.mean(), bayesian_scores.std()))\n",
    "mse_score(bayesian_y_pred, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_bayesian_model(y_pred, y): \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(y[:60], label='True values')\n",
    "    plt.plot(y_pred[:60], label='Predicted values')\n",
    "    plt.xlabel('Number of samples')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend(prop=dict(size=10))\n",
    "    plt.title('Bayesian ridge regression: comparison of predicted values with true values')\n",
    "    plt.show()\n",
    "plot_bayesian_model(bayesian_y_pred, y)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_weights(bayesian_model_weights, linear_regression_model_weights):\n",
    "    lw = 2\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.title(\"Weights of the model\")\n",
    "    plt.plot(bayesian_model.coef_, color='lightgreen', linewidth=lw, label='Bayesian Ridge estimate')\n",
    "    plt.plot(linear_regression_model.coef_, color='navy', linestyle='--', label='OLS estimate')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Value of the weights')\n",
    "    plt.legend(loc='best', prop=dict(size=12))\n",
    "    plt.show()\n",
    "plot_weights(bayesian_model.coef_, linear_regression_model.coef_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Compare scores\n",
    "def compare_regression_scores(bayesian_scores, lin_reg_scores):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('Compare scores')\n",
    "    plt.plot(bayesian_scores, label='Bayesian ridge regression')\n",
    "    plt.plot(lin_reg_scores, label='Linear regression')\n",
    "    plt.xlabel('Number of scores')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(loc='best', prop=dict(size=10))\n",
    "    plt.show()\n",
    "compare_regression_scores(bayesian_scores, lin_reg_scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Apply polynomial degree of 2 features to Linear Regression mode\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    polynomial_features = PolynomialFeatures(degree=2)\n",
    "    X_poly = polynomial_features.fit_transform(X)\n",
    "    linear_regression_model.fit(X_poly, y)\n",
    "    y_poly_pred = linear_regression_model.predict(X_poly)\n",
    "    mse_score(y_poly_pred, y)\n",
    "    return y_poly_pred, make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))\n",
    "\n",
    "PolynomialRegression(X)\n",
    "polynomial_regression_scores = cross_val_score(linear_regression_model, X, y, cv=10)\n",
    "print(f'Polynomial regression scores: {polynomial_regression_scores}')\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (polynomial_regression_scores.mean(), polynomial_regression_scores.std()))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(y[:60], label='True values')\n",
    "plt.plot(y_poly_pred[:60], label='Predicted values')\n",
    "plt.xlabel('Number of samples')\n",
    "plt.ylabel('Values')\n",
    "plt.legend(prop=dict(size=10))\n",
    "plt.title('Polynomial regression: comparison of predicted values with true values')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Compare scores between 3 different model outputs\n",
    "def compare_regression_scores(polynomial_regression_scores, bayesian_scores, lin_reg_scores):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.title('Compare scores')\n",
    "    plt.plot(polynomial_regression_scores, label='Polynomial regression')\n",
    "    plt.plot(bayesian_scores, label='Bayesian ridge regression')\n",
    "    plt.plot(lin_reg_scores, label='Linear regression')\n",
    "    plt.xlabel('Number of scores')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(loc='best', prop=dict(size=10))\n",
    "    plt.show()\n",
    "compare_regression_scores(polynomial_regression_scores, bayesian_scores, lin_reg_scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Check cross validation score on VALIDATION set\n",
    "lin_reg_test_scores = cross_val_score(linear_regression_model, X_val, y_val, cv=10)\n",
    "param_range = np.logspace(-7, 3, 3)\n",
    "train_scores, validation_scores = validation_curve(BayesianRidge(), X, y, param_name='alpha_1', param_range=param_range, cv=10)\n",
    "indexed_train_scores = train_scores[1]\n",
    "\n",
    "train_scores_mean = np.mean(indexed_train_scores)\n",
    "train_scores_std = np.std(indexed_train_scores)\n",
    "test_scores_mean = np.mean(lin_reg_test_scores)\n",
    "test_scores_std = np.std(lin_reg_test_scores)\n",
    "print(f'Train scores mean: {train_scores_mean}. Train scores std: {train_scores_std}. Test scores mean: {test_scores_mean}. Test scores std: {test_scores_std}.')\n",
    "plt.title(\"Validation Curve with Linear Regression\")\n",
    "plt.xlabel(r\"$\\gamma$\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.semilogx(train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.fill_between(train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                 color=\"darkorange\", lw=lw)\n",
    "plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                 color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param = {\n",
    "    'polynomialfeatures__degree': np.arange(10), 'linearregression__fit_intercept': [True, False], 'linearregression__normalize': [True, False]\n",
    "}\n",
    "\n",
    "bayesian_scores = cross_val_score(s(), X, y, cv=10)\n",
    "# poly_grid = GridSearchCV(bayesian_model, param)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.1",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "569d6b7e9215e11aba41c6454007e5c1b78bad7df09dab765d8cf00362c40f03"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}